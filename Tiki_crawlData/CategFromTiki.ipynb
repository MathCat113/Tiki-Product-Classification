{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawling from a search bar\n",
    "# returns list of urls of product in search result\n",
    "def search_bar_crawler(link, crawl_limit):\n",
    "    html = urlopen(link)\n",
    "    soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "    # with io.open('html.txt', 'w', encoding='utf8') as file:\n",
    "    #     file.write(str(soup))\n",
    "\n",
    "    # need to compile a list of item href\n",
    "    script_tags = soup.find_all(\"script\", type=\"application/ld+json\")\n",
    "\n",
    "    # Create a list to store the product URLs\n",
    "    product_urls = []\n",
    "\n",
    "    # Iterate over the `<script type=\"application/ld+json\">` tags\n",
    "    for script_tag in script_tags:\n",
    "\n",
    "        # Extract the JSON text from the `<script type=\"application/ld+json\">` tag\n",
    "        json_text = script_tag.text\n",
    "\n",
    "        # Load the JSON text into a Python object\n",
    "        json_object = json.loads(json_text)\n",
    "\n",
    "        # Check if the JSON object has the \"@type\": \"Product\" property\n",
    "        if json_object[\"@type\"] == \"Product\":\n",
    "\n",
    "            # Get the \"href\" attribute of the JSON object\n",
    "            product_url = json_object[\"url\"]\n",
    "\n",
    "            # Add the product URL to the list\n",
    "            product_urls.append(product_url)\n",
    "        \n",
    "        # arbitrary stop sign (i can think of a million ways this can go wrong)\n",
    "        # 10 is too much\n",
    "        if len(product_urls) == crawl_limit:\n",
    "            return product_urls\n",
    "    \n",
    "    return product_urls\n",
    "\n",
    "# print(search_bar_crawler('https://tiki.vn/search?q=adidas'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific product crawling (categories)\n",
    "# returns all category tags from list of urls\n",
    "def get_tiki_category(product_urls):\n",
    "    category_tags = []\n",
    "    count = 0\n",
    "    for product_url in product_urls:\n",
    "        # arbitrary stop sign to avoid RemoteDisconnected\n",
    "\n",
    "        productHTML = urlopen(product_url)\n",
    "        soup = BeautifulSoup(productHTML.read(), 'html.parser')\n",
    "\n",
    "        breadcrumb_items = soup.find_all(\"a\", class_=\"breadcrumb-item\")\n",
    "\n",
    "        # Extract all spans within the <a> tags\n",
    "        breadcrumb_spans = []\n",
    "        for breadcrumb_item in breadcrumb_items:\n",
    "            breadcrumb_spans.extend(breadcrumb_item.find_all(\"span\"))\n",
    "\n",
    "        breadcrumb_texts = [bc.text for bc in breadcrumb_items[0: len(breadcrumb_items) - 1]]\n",
    "\n",
    "        # Print the breadcrumb spans\n",
    "        category_tag = ' > '.join(breadcrumb_texts)\n",
    "\n",
    "        # print(category_tag)\n",
    "        category_tags.append(category_tag)\n",
    "    \n",
    "    return category_tags\n",
    "\n",
    "# adidas_category_tags = get_tiki_category(search_bar_crawler('https://tiki.vn/search?q=adidas'))\n",
    "# print(adidas_category_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a really simple function to count the occurences of each category tag\n",
    "def list_to_dict_count(list1):\n",
    "    \"\"\"Returns a dictionary with unique elements of the given list as keys and their count of occurrence as values.\n",
    "\n",
    "    Args:\n",
    "        list1: A Python list.\n",
    "\n",
    "    Returns:\n",
    "        A Python dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    dict1 = {}\n",
    "    for element in list1:\n",
    "        if element in dict1:\n",
    "            dict1[element] += 1\n",
    "        else:\n",
    "            dict1[element] = 1\n",
    "    return dict1\n",
    "\n",
    "# print(list_to_dict_with_unique_elements_and_count(adidas_category_tags))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HB Gift Box 1 - Bia đóng hộp làm quà tặng sang trọng']\n"
     ]
    }
   ],
   "source": [
    "# compile the datasheet into corresponding classes (build by dict)\n",
    "def compile_df_to_dict_with_unique_label_values(df):\n",
    "    # Get the unique label values.\n",
    "    unique_label_values = df['Label'].unique()\n",
    "\n",
    "    # Create a dictionary to store the results.\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over the unique label values and add them to the dictionary.\n",
    "    for label_value in unique_label_values:\n",
    "        # Get the product name for the current label value.\n",
    "        product_name = df.loc[df['Label'] == label_value, 'Product Name'].tolist()\n",
    "\n",
    "        # Add the label value and product name to the dictionary.\n",
    "        results[label_value] = product_name\n",
    "\n",
    "    # Return the dictionary.\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "df = pd.read_excel('data_output.xlsx')\n",
    "# unique_labels = set(df['Label'])\n",
    "# print(len(unique_labels))\n",
    "\n",
    "# Compile the DataFrame into a dictionary.\n",
    "labelDict = compile_df_to_dict_with_unique_label_values(df)\n",
    "\n",
    "\n",
    "# Print the dictionary.\n",
    "# print(len(labelDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chụp khí Hypertherm 220977']\n"
     ]
    }
   ],
   "source": [
    "# link_format = 'https://tiki.vn/search?q='\n",
    "# categs = []\n",
    "\n",
    "# for product in labelDict['__Ghế_văn_phòng'][0:4]:\n",
    "#     encoded_string = urllib.parse.quote_plus(product)\n",
    "#     link = link_format + encoded_string\n",
    "#     categ = get_tiki_category(search_bar_crawler(link))\n",
    "#     categs.extend(categ)\n",
    "\n",
    "# # there should be 10 per link\n",
    "# print(len(categs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_count = list_to_dict_count(categs)\n",
    "# print(len(categs))\n",
    "\n",
    "\n",
    "# sorted_dict = sorted(dict_count.items(), key=lambda item: item[1], reverse= True)\n",
    "\n",
    "# # Print the sorted dictionary\n",
    "# print(sorted_dict)\n",
    "# most_popular_categ = sorted_dict[0][0]\n",
    "# pop_ratio = sorted_dict[0][1] / len(categs)\n",
    "# second_most_popular_categ = sorted_dict[1][0]\n",
    "# second_pop_ratio = sorted_dict[1][1] / len(categs)\n",
    "# print(most_popular_categ)\n",
    "# print(pop_ratio)\n",
    "# print(second_most_popular_categ)\n",
    "# print(second_pop_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first wave of tiki scraping\n",
    "\n",
    "strengths:\n",
    "    all labels (1971) have been looked into\n",
    "    \n",
    "weaknesses:\n",
    "    those of low ratio\n",
    "\n",
    "    duplicates\n",
    "\n",
    "    -> needs to be compiled and analyzed via pandas\n",
    "    \n",
    "    theres too many of 'em, redo it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfirst wave of tiki scraping\\nstrengths:\\n    all labels (1971) have been looked into\\nweaknesses:\\n    those of low ratio\\n    duplicates\\n    -> needs to be compiled and analyzed via pandas\\n    theres too many of 'em, redo it\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# to ensure that I don't have to read the same thing all over, I check all the read labels, and remove them from the labelDict\n",
    "\n",
    "# find list of read labels\n",
    "# with open('categories_ratio.txt', 'r', encoding='utf-8') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# labelLines = []\n",
    "# for i, line in enumerate(lines):\n",
    "#     if i % 3 == 0:\n",
    "#         labelLines.append(line.replace('\\n', ''))\n",
    "\n",
    "# # print(label_lines)\n",
    "\n",
    "# unreadLabelDict = {}\n",
    "# for key, value in labelDict.items():\n",
    "#     if key not in labelLines:\n",
    "#         unreadLabelDict[key] = value \n",
    "\n",
    "# print(unreadLabelDict)\n",
    "# link_format = 'https://tiki.vn/search?q='\n",
    "# for key in unreadLabelDict:\n",
    "#     categs = []\n",
    "#     count = 0\n",
    "#     for product in unreadLabelDict[key]:\n",
    "#         # dumbest error ever\n",
    "#         encoded_string = urllib.parse.quote_plus(product.replace('∅', ''))\n",
    "#         link = link_format + encoded_string\n",
    "#         # print(link)\n",
    "#         categ = \"\"\n",
    "#         try:\n",
    "#             categ = get_tiki_category(search_bar_crawler(link))\n",
    "#         except:\n",
    "#             time.sleep(5)\n",
    "#             categ = get_tiki_category(search_bar_crawler(link))\n",
    "            \n",
    "#         categs.extend(categ)\n",
    "#         if count >= 20:\n",
    "#             time.sleep(3)\n",
    "#             count = 0\n",
    "#         else:\n",
    "#             count += len(categ)\n",
    "    \n",
    "#     dict_count = list_to_dict_count(categs)\n",
    "#     sorted_dict = sorted(dict_count.items(), key=lambda item: item[1], reverse= True)\n",
    "\n",
    "#     # Print the sorted dictionary\n",
    "#     most_popular_categ = sorted_dict[0][0]\n",
    "#     pop_ratio = sorted_dict[0][1] / len(categs)\n",
    "#     with open('categories_ratio.txt', 'a', encoding='utf-8') as f:\n",
    "#         f.write(str(key) + '\\n')\n",
    "#         f.write(most_popular_categ + '\\n')\n",
    "#         f.write(str(pop_ratio) + '\\n')\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick program pushing categories_ratio.txt into a xlsx, for ease of analysis\n",
    "\n",
    "failed, bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA quick program pushing categories_ratio.txt into a xlsx, for ease of analysis\\nfailed, bad data\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# with open('categories_ratio.txt', 'r', encoding= 'utf-8') as f:\n",
    "#     lines = f.readlines()\n",
    "#     labels = []\n",
    "#     categs = []\n",
    "#     weight = []\n",
    "#     for i, line in enumerate(lines):\n",
    "#         if i % 3 == 0:\n",
    "#             labels.append(line.replace('\\n', ''))\n",
    "#         elif i % 3 == 1:\n",
    "#             categs.append(line.replace('\\n', ''))\n",
    "#         else:\n",
    "#             weight.append(line.replace('\\n', ''))\n",
    "    \n",
    "#     weights = [float(x) for x in weight]\n",
    "#     print(len(weights))\n",
    "\n",
    "#     df = pd.DataFrame(list(zip(labels, categs, weights)), columns= ['Label', 'Category', 'Weight'])\n",
    "#     df.to_excel('first_wave_tiki_crawl.xlsx')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open('categories_ratio_wave_2.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# add another column: size of label\n",
    "\n",
    "labelLines = []\n",
    "for i, line in enumerate(lines):\n",
    "    if i % 4 == 0:\n",
    "        labelLines.append(line.replace('\\n', ''))\n",
    "\n",
    "# print(label_lines)\n",
    "\n",
    "unreadLabelDict = {}\n",
    "for key, value in labelDict.items():\n",
    "    if key not in labelLines:\n",
    "        unreadLabelDict[key] = value \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wave 2, re-crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_format = 'https://tiki.vn/search?q='\n",
    "# for i, key in enumerate(unreadLabelDict):\n",
    "    \n",
    "#     categs = []\n",
    "#     count = 0\n",
    "#     limit = 20\n",
    "#     HTTPErrorCount = 0\n",
    "#     IndexErrorCount = 0\n",
    "#     UnicodeErrorCount = 0\n",
    "#     if (len(unreadLabelDict[key]) >= 50):\n",
    "#         limit = 5\n",
    "#     elif (len(unreadLabelDict[key]) >= 20):\n",
    "#         limit = 10\n",
    "\n",
    "#     for product in unreadLabelDict[key]:\n",
    "#         # dumbest error ever\n",
    "#         encoded_string = urllib.parse.quote_plus(product.replace('∅', ''))\n",
    "#         link = link_format + encoded_string\n",
    "#         link = link.replace('∅', '')\n",
    "#         # print(link)\n",
    "#         categ = \"\"\n",
    "\n",
    "#         try:\n",
    "#             categ = get_tiki_category(search_bar_crawler(link, crawl_limit= limit))\n",
    "#         except HTTPError:\n",
    "#             print(key + ' HTTP ERROR: ' + link)\n",
    "#             HTTPErrorCount += 1\n",
    "#             print(HTTPErrorCount)\n",
    "#             continue\n",
    "#         except UnicodeEncodeError:\n",
    "#             print(key + ' UNICODE ERROR: ' + link)\n",
    "#             UnicodeErrorCount += 1\n",
    "#             print(UnicodeErrorCount)\n",
    "#             continue\n",
    "#         except:\n",
    "#             time.sleep(5)\n",
    "#             categ = get_tiki_category(search_bar_crawler(link, crawl_limit= limit))\n",
    "            \n",
    "#         categs.extend(categ)\n",
    "#         if count >= 20:\n",
    "#             time.sleep(3)\n",
    "#             count = 0\n",
    "#         else:\n",
    "#             count += len(categ)\n",
    "        \n",
    "    \n",
    "#     dict_count = list_to_dict_count(categs)\n",
    "#     sorted_dict = sorted(dict_count.items(), key=lambda item: item[1], reverse= True)\n",
    "\n",
    "#     # Print the sorted dictionary\n",
    "#     try:\n",
    "#         most_popular_categ = sorted_dict[0][0]\n",
    "#         pop_ratio = sorted_dict[0][1] / len(categs)\n",
    "#         with open('categories_ratio_wave_2.txt', 'a', encoding='utf-8') as f:\n",
    "#             f.write(str(key) + '\\n')\n",
    "#             f.write(str(len(unreadLabelDict[key])) + '\\n')\n",
    "#             f.write(most_popular_categ + '\\n')\n",
    "#             f.write(str(pop_ratio) + '\\n')\n",
    "#     except IndexError:\n",
    "#             print(key + ' INDEX ERROR: ' + link)\n",
    "#             IndexErrorCount += 1\n",
    "#             print(IndexErrorCount)\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling second wave results into another excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1971\n",
      "1971\n"
     ]
    }
   ],
   "source": [
    "# with open('categories_ratio_wave_2.txt', 'r', encoding= 'utf-8') as f:\n",
    "#     lines = f.readlines()\n",
    "#     labels = []\n",
    "#     label_size = []\n",
    "#     categs = []\n",
    "#     weight = []\n",
    "#     for i, line in enumerate(lines):\n",
    "#         if i % 4 == 0:\n",
    "#             labels.append(line.replace('\\n', ''))\n",
    "#         elif i % 4 == 1:\n",
    "#             label_size.append(line.replace('\\n', ''))\n",
    "#         elif i % 4 == 2:\n",
    "#             categs.append(line.replace('\\n', ''))\n",
    "#         else:\n",
    "#             weight.append(line.replace('\\n', ''))\n",
    "    \n",
    "#     weights = [float(x) for x in weight]\n",
    "#     label_sizes = [int(x) for x in label_size]\n",
    "#     print(len(weights))\n",
    "\n",
    "#     df = pd.DataFrame(list(zip(labels, label_sizes, categs, weights)), columns= ['Label', 'Label Size', 'Category', 'Weight'])\n",
    "#     df.to_excel('second_wave_tiki_crawl.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                        Label  Label Size  \\\n",
      "0           0              __Ghế_văn_phòng         129   \n",
      "1           1                  __Thước_dây           8   \n",
      "2           2                    __Cưa_tay           2   \n",
      "3           3                   __Váy,_đầm         158   \n",
      "4           4  __Dụng_cụ_đo,_kiểm_tra_khác          55   \n",
      "\n",
      "                                            Category    Weight  \n",
      "0  Trang chủ > Nhà Cửa - Đời Sống > Nội thất > Nộ...  0.759690  \n",
      "1  Trang chủ > Nhà Cửa - Đời Sống > Sửa chữa nhà ...  1.000000  \n",
      "2  Trang chủ > Nhà Cửa - Đời Sống > Sửa chữa nhà ...  0.250000  \n",
      "3  Trang chủ > Thời trang nữ > Đầm nữ > Đầm Dáng Xòe  0.437975  \n",
      "4  Trang chủ > Nhà Cửa - Đời Sống > Sửa chữa nhà ...  0.370909  \n",
      "776\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('second_wave_tiki_crawl.xlsx')\n",
    "print(df.head(5))\n",
    "print(len(df['Category'].unique()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
